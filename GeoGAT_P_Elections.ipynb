{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Pq2buRX5BFUn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.neighbors import BallTree\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.nn.norm import BatchNorm\n",
        "from torch.utils.data import random_split\n",
        "import libpysal\n",
        "from libpysal.weights import Queen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRlpFFsNBHy_",
        "outputId": "b702bfea-d0ef-40e2-aaf7-40eb4bd285cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sKcO69a3BJ52"
      },
      "outputs": [],
      "source": [
        "data = gpd.read_file(\"/data/election.geojson\")\n",
        "# Lambert Conformal Conic\n",
        "data = data.to_crs(\"+proj=lcc +lat_1=33 +lat_2=45 +lat_0=39 +lon_0=-96 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\")\n",
        "\n",
        "y = data['pct_gop_16'].values\n",
        "\n",
        "X_vars = ['AGE135214','AGE775214','POP815213',\n",
        "          'SEX255214','RHI125214','POP715213','EDU635213','EDU685213',\n",
        "          'LFE305213','HSG445213']\n",
        "X = data[X_vars].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "features = torch.FloatTensor(X_scaled).to(device)\n",
        "labels = torch.FloatTensor(y).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdjVYCErBNA-",
        "outputId": "24389e78-fb4b-45b3-f8b3-8044fd49cf08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-166-f633dfe526c9>:17: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  queen_w = Queen.from_dataframe(data)\n"
          ]
        }
      ],
      "source": [
        "def calculate_weighted_similarity(X, sigma_values, weights):\n",
        "    u = X[:, np.newaxis, :]\n",
        "    v = X[np.newaxis, :, :]\n",
        "    sq_diff = (u - v) ** 2\n",
        "\n",
        "    E_i = np.exp(-((sq_diff) / (2 * (sigma_values))))\n",
        "\n",
        "    weighted_similarity = np.average(E_i, axis=2, weights=weights)\n",
        "    return weighted_similarity\n",
        "\n",
        "sigma_values = np.var(X, axis=0)\n",
        "weights = np.array([0.393, 0.3, 1.02, 0.511, 1.921, 0.276, 0.5388, 2.356, 0.589, 1.06])\n",
        "# weights = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
        "\n",
        "base_similarity_matrix = calculate_weighted_similarity(X, sigma_values, weights)\n",
        "\n",
        "queen_w = Queen.from_dataframe(data)\n",
        "\n",
        "# Compute neighbor mean vectors based on Queen rule\n",
        "def compute_neighbor_mean_vectors(X, neighbors_dict):\n",
        "    N, D = X.shape\n",
        "    neighbor_means = np.zeros((N, D))\n",
        "    for i in range(N):\n",
        "        nbrs = neighbors_dict[i]\n",
        "        if len(nbrs) == 0:\n",
        "            neighbor_means[i] = X[i]\n",
        "        else:\n",
        "            neighbor_means[i] = np.mean(X[nbrs], axis=0)\n",
        "    return neighbor_means\n",
        "\n",
        "neighbor_means = compute_neighbor_mean_vectors(X_scaled, queen_w.neighbors)\n",
        "\n",
        "# Calculae the neighorhood similarity\n",
        "def calculate_neighbor_similarity(neighbor_means, sigma_values, weights):\n",
        "    u = neighbor_means[:, np.newaxis, :]\n",
        "    v = neighbor_means[np.newaxis, :, :]\n",
        "\n",
        "    sq_diff = (u - v) ** 2\n",
        "\n",
        "    E_i = np.exp(-((sq_diff) / (2 * sigma_values)))\n",
        "    weighted_neighbor_sim = np.average(E_i, axis=2, weights=weights)\n",
        "    return weighted_neighbor_sim\n",
        "\n",
        "neighbor_similarity_matrix = calculate_neighbor_similarity(neighbor_means, sigma_values, weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "14pqAuj-PUct"
      },
      "outputs": [],
      "source": [
        "# S_l +S_n: α * base_similarity + β * neighbor_similarity\n",
        "\n",
        "alpha = 1.0\n",
        "beta = 1.0\n",
        "combined_similarity_matrix = alpha * base_similarity_matrix + beta * neighbor_similarity_matrix\n",
        "\n",
        "\n",
        "dist_matrix = squareform(pdist(combined_similarity_matrix, metric='euclidean'))\n",
        "nearest_neighbors = np.argsort(dist_matrix, axis=1)[:, 1:31]\n",
        "\n",
        "adj = np.zeros((X.shape[0], X.shape[0]))\n",
        "for i in range(nearest_neighbors.shape[0]):\n",
        "    for j in nearest_neighbors[i]:\n",
        "        adj[i, j] = combined_similarity_matrix[i, j]\n",
        "adj = (adj + adj.T) / 2\n",
        "\n",
        "adj = torch.FloatTensor(adj).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "Bl9P-kjwBQ0_"
      },
      "outputs": [],
      "source": [
        "dataset = TensorDataset(features, labels)\n",
        "train_size = int(0.1 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "all_indices = np.arange(len(dataset))\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(all_indices)\n",
        "\n",
        "train_indices = all_indices[:train_size]\n",
        "val_indices = all_indices[train_size:train_size + val_size]\n",
        "test_indices = all_indices[train_size + val_size:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "  def __init__(self, f_in, n_classes, hidden=[128], heads=18, dropout=0.2):\n",
        "    super(GAT, self).__init__()\n",
        "    self.conv1 = GATConv(f_in, hidden[0], heads=heads, dropout=dropout, edge_dim=1)\n",
        "    self.bn1 = BatchNorm(hidden[0] * heads)\n",
        "    self.conv2 = GATConv(hidden[0] * heads, n_classes, heads=heads, concat=False, dropout=dropout, edge_dim=1)\n",
        "    self.bn2 = BatchNorm(n_classes)\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def forward(self, x, edge_index, edge_attr):\n",
        "    x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    x = F.elu(self.conv1(x, edge_index, edge_attr))\n",
        "    x = self.bn1(x)\n",
        "    x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    x = self.conv2(x, edge_index, edge_attr)\n",
        "    x = self.bn2(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "edge_index = torch_geometric.utils.dense_to_sparse(adj)[0].to(device)\n",
        "edge_attr = adj[edge_index[0], edge_index[1]].to(device)\n",
        "\n",
        "def filter_edge_index(edge_index, edge_attr, indices):\n",
        "    mask = torch.isin(edge_index, torch.tensor(indices).to(device)).all(dim=0)\n",
        "    filtered_edge_index = edge_index[:, mask]\n",
        "    filtered_edge_attr = edge_attr[mask]\n",
        "    remap = {old_idx: new_idx for new_idx, old_idx in enumerate(indices)}\n",
        "    remapped_edge_index = torch.stack([\n",
        "        torch.tensor([remap[int(i)] for i in filtered_edge_index[0]]).to(device),\n",
        "        torch.tensor([remap[int(i)] for i in filtered_edge_index[1]]).to(device)\n",
        "    ])\n",
        "    return remapped_edge_index, filtered_edge_attr"
      ],
      "metadata": {
        "id": "xI3bnyaJWMkn"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "uF36A92VBXXH"
      },
      "outputs": [],
      "source": [
        "def train_model(model, features, edge_index, edge_attr, labels, train_indices, val_indices, epochs=2000, lr=1e-3):\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=1e-5)\n",
        "    model.train()\n",
        "    best_val_loss = float('inf')\n",
        "    early_stopping_patience = 20\n",
        "    train_edge_index, train_edge_attr = filter_edge_index(edge_index, edge_attr, train_indices)\n",
        "    val_edge_index, val_edge_attr = filter_edge_index(edge_index, edge_attr, val_indices)\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(features[train_indices], train_edge_index, train_edge_attr)\n",
        "        loss = F.mse_loss(output, labels[train_indices].unsqueeze(1))\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_output = model(features[val_indices], val_edge_index, val_edge_attr)\n",
        "            val_loss = F.mse_loss(val_output, labels[val_indices].unsqueeze(1))\n",
        "        model.train()\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n",
        "\n",
        "        if patience >= early_stopping_patience:\n",
        "            print('Early stopping.')\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5HtdjhgMD_o",
        "outputId": "5c05797a-0870-4656-fb3a-20ba5dd06272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/2000, Training Loss: 0.9250, Validation Loss: 0.2579\n",
            "Epoch 20/2000, Training Loss: 0.6790, Validation Loss: 0.2216\n",
            "Epoch 30/2000, Training Loss: 0.4893, Validation Loss: 0.2067\n",
            "Epoch 40/2000, Training Loss: 0.3416, Validation Loss: 0.1615\n",
            "Epoch 50/2000, Training Loss: 0.2332, Validation Loss: 0.1252\n",
            "Epoch 60/2000, Training Loss: 0.1601, Validation Loss: 0.0830\n",
            "Epoch 70/2000, Training Loss: 0.1035, Validation Loss: 0.0552\n",
            "Epoch 80/2000, Training Loss: 0.0669, Validation Loss: 0.0366\n",
            "Epoch 90/2000, Training Loss: 0.0456, Validation Loss: 0.0266\n",
            "Epoch 100/2000, Training Loss: 0.0324, Validation Loss: 0.0199\n",
            "Epoch 110/2000, Training Loss: 0.0219, Validation Loss: 0.0153\n",
            "Epoch 120/2000, Training Loss: 0.0164, Validation Loss: 0.0128\n",
            "Epoch 130/2000, Training Loss: 0.0138, Validation Loss: 0.0114\n",
            "Epoch 140/2000, Training Loss: 0.0108, Validation Loss: 0.0108\n",
            "Epoch 150/2000, Training Loss: 0.0094, Validation Loss: 0.0104\n",
            "Epoch 160/2000, Training Loss: 0.0091, Validation Loss: 0.0102\n",
            "Epoch 170/2000, Training Loss: 0.0095, Validation Loss: 0.0102\n",
            "Epoch 180/2000, Training Loss: 0.0095, Validation Loss: 0.0101\n",
            "Epoch 190/2000, Training Loss: 0.0088, Validation Loss: 0.0101\n",
            "Epoch 200/2000, Training Loss: 0.0088, Validation Loss: 0.0102\n",
            "Early stopping.\n"
          ]
        }
      ],
      "source": [
        "model = GAT(f_in=features.shape[1], n_classes=1, hidden=[40], heads=20, dropout=0.2).to(device)\n",
        "trained_model = train_model(model, features, edge_index, edge_attr, labels, train_indices, val_indices, epochs=2000, lr=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, features, edge_index, edge_attr, labels, indices):\n",
        "    model.eval()\n",
        "    test_edge_index, test_edge_attr = filter_edge_index(edge_index, edge_attr, indices)\n",
        "    with torch.no_grad():\n",
        "        predictions = model(features[indices], test_edge_index, test_edge_attr)\n",
        "        if isinstance(predictions, tuple):\n",
        "            predictions = predictions[0]\n",
        "        predictions = predictions.squeeze()\n",
        "        mse = F.mse_loss(predictions, labels[indices].squeeze())\n",
        "        mae = F.l1_loss(predictions, labels[indices].squeeze())\n",
        "        y_true = labels[indices].cpu().numpy()\n",
        "        y_pred = predictions.cpu().numpy()\n",
        "    return predictions, mse, mae\n",
        "\n",
        "predictions, mse, mae = evaluate_model(trained_model, features, edge_index, edge_attr, labels, test_indices)\n",
        "print(f'Test MSE: {mse.item():.4f}')\n",
        "print(f'Test MAE: {mae.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqC79FF1XIsR",
        "outputId": "ea0dfa03-f468-4e9f-fde4-4ebbba5a825f"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 0.0105\n",
            "Test MAE: 0.0789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y9jy8BzeedvL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNObjZUisLCf3kODarO13l9"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}